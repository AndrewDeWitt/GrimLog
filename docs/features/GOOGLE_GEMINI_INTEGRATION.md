# Google Gemini Integration

**Added:** November 4, 2025
**Version:** 4.1.0

## Overview

Grimlog now supports **Google Gemini Flash 2.5** as an alternative AI provider alongside OpenAI. You can switch between providers using a simple environment variable, with no code changes required.

## Supported Models

### OpenAI (Default)
- **Analysis**: `gpt-5-mini` - Main game analysis and tool calling
- **Intent Classification**: `gpt-5-nano` - Fast intent classification and gatekeeper

### Google Gemini
- **Analysis**: `gemini-2.5-flash` - Main game analysis and tool calling
- **Intent Classification**: `gemini-2.5-flash` - Fast intent classification and gatekeeper

## Configuration

### Option 1: Use OpenAI (Default)

```bash
# .env.local
AI_PROVIDER=openai  # or omit entirely (defaults to openai)
OPENAI_API_KEY=sk-...
```

### Option 2: Use Google Gemini

```bash
# .env.local
AI_PROVIDER=google
GOOGLE_API_KEY=...  # Get from https://aistudio.google.com/
```

## Getting a Google AI API Key

1. Go to [Google AI Studio](https://aistudio.google.com/)
2. Sign in with your Google account
3. Click **"Get API Key"**
4. Create a new API key for your project
5. Copy the key to your `.env.local` file

## Features

### Structured Output Support

Both providers support structured JSON outputs:
- **Intent Classification**: Returns JSON with `isGameRelated`, `intent`, `contextTier`, `confidence`, and `reasoning`
- **Tool Calling**: Returns function calls with structured parameters for all 24 AI tools

### All 24 AI Tools Supported

The following tools work with both providers:

**State Management:**
- `change_phase`
- `change_player_turn`
- `advance_battle_round`
- `query_game_state`

**Resource Tracking:**
- `update_command_points`
- `update_victory_points`

**Unit Operations:**
- `update_unit_health`
- `mark_unit_destroyed`
- `update_unit_status`
- `update_objective_control`

**Combat & Actions:**
- `log_combat_result`
- `log_unit_action`
- `log_stratagem_use`

**Secondary Objectives:**
- `set_secondary_objectives`
- `redraw_secondary_objective`
- `score_secondary_vp`
- `score_assassination`
- `score_bring_it_down`
- `score_marked_for_death`
- `score_no_prisoners`
- `score_cull_the_horde`
- `score_overwhelming_force`

**Strategic:**
- `get_strategic_advice`

## Implementation Details

### Architecture

The implementation uses a **provider abstraction layer** with separate tool definitions for each provider:

```
lib/
â”œâ”€â”€ aiProvider.ts           # Provider selection and normalization
â”œâ”€â”€ aiTools.ts              # OpenAI tool definitions
â”œâ”€â”€ aiToolsGemini.ts        # Gemini tool definitions
â”œâ”€â”€ intentOrchestrator.ts   # Intent classification (both providers)
â””â”€â”€ toolHandlers.ts         # Tool execution (provider-agnostic)
```

### Provider Abstraction

The `lib/aiProvider.ts` module provides:
- `getProvider()` - Returns 'openai' or 'google' based on env var
- `validateProviderConfig()` - Checks that required API keys are present
- `getModelName()` - Returns the appropriate model name
- `extractOpenAIFunctionCalls()` - Normalizes OpenAI responses
- `extractGeminiFunctionCalls()` - Normalizes Gemini responses

### Tool Definitions

**OpenAI Format** (`lib/aiTools.ts`):
```typescript
{
  type: "function",
  name: "change_phase",
  parameters: {
    type: "object",
    properties: {
      new_phase: {
        type: "string",
        enum: ["Command", "Movement", "Shooting", "Charge", "Fight"]
      }
    }
  }
}
```

**Gemini Format** (`lib/aiToolsGemini.ts`):
```typescript
{
  name: "change_phase",
  parameters: {
    type: Type.OBJECT,
    properties: {
      new_phase: {
        type: Type.STRING,
        enum: ["Command", "Movement", "Shooting", "Charge", "Fight"]
      }
    }
  }
}
```

### Response Normalization

Both providers return function calls in different formats. The abstraction layer normalizes them to:

```typescript
interface NormalizedFunctionCall {
  name: string;           // Function name
  arguments: string;      // JSON string of arguments
  call_id: string;        // Unique call identifier
}
```

## Usage

### Switching Providers

Simply change the `AI_PROVIDER` environment variable and restart your server:

```bash
# Switch to Google Gemini
AI_PROVIDER=google npm run dev

# Switch back to OpenAI
AI_PROVIDER=openai npm run dev
```

### Monitoring

The system logs which provider is being used:

```
ðŸ¤– Using AI Provider: GOOGLE
ðŸ“Š Model: gemini-2.5-flash
```

### Langfuse Tracing

Both providers are fully integrated with Langfuse for observability:
- Provider name logged in metadata
- Model name tracked per request
- Tool calls traced with provider context
- Errors logged with provider information

## Error Handling

### Missing API Keys

If you don't have the required API key for the selected provider:

```
Provider configuration error: GOOGLE_API_KEY environment variable is required when AI_PROVIDER=google
```

The system will return a fallback response instead of crashing.

### Rate Limits

Both providers have retry logic with exponential backoff:
- Maximum 3 retry attempts
- Backoff: 100ms, 200ms, 400ms

### Graceful Degradation

If the AI provider fails:
1. Error is logged
2. Langfuse trace updated with error
3. User receives transcription with no tool calls
4. Game continues without AI analysis

## Cost Comparison

### OpenAI Pricing (as of Nov 2024)
- **GPT-5-mini**: TBD (newer model)
- **GPT-5-nano**: TBD (newer model)

### Google Gemini Pricing (as of Nov 2024)
- **Gemini 2.5 Flash**: Free tier available
- **Structured output**: No additional cost
- **Function calling**: No additional cost

Check current pricing:
- [OpenAI Pricing](https://openai.com/pricing)
- [Google AI Pricing](https://ai.google.dev/pricing)

## Performance

Both providers have similar performance characteristics:

| Provider | Intent Classification | Analysis | Total |
|----------|---------------------|----------|-------|
| OpenAI   | ~200-400ms          | ~800-1200ms | ~1-1.6s |
| Google   | ~300-500ms          | ~700-1100ms | ~1-1.6s |

*Times may vary based on network, load, and request complexity*

## Limitations

### Gemini-Specific

1. **No system instruction parameter** - System prompts are prepended to user content
2. **Different schema format** - Uses `Type` enum instead of string types
3. **No call IDs** - Generated client-side with `crypto.randomUUID()`

### OpenAI-Specific

1. **Responses API** - Uses newer API format, not Chat Completions
2. **Requires reasoning effort** - Must specify effort level for o-series models
3. **Verbosity control** - Different from standard temperature parameter

## Testing

### Validation

The implementation has been validated with:
- âœ… TypeScript compilation (`npx tsc --noEmit`)
- âœ… ESLint linting (`npm run lint`)
- âœ… Type safety across all files
- âœ… Provider abstraction tests

### Manual Testing Checklist

When testing with actual API keys:
- [ ] Intent classification works with both providers
- [ ] Tool calling works with both providers
- [ ] All 24 tools execute correctly
- [ ] Langfuse traces show provider information
- [ ] Error handling works without API key
- [ ] Switching providers doesn't break session
- [ ] Retry logic works on failures

## Troubleshooting

### "systemInstruction does not exist"

This is expected - Gemini uses a different API format. The system prepends the prompt to the user content instead.

### "Property 'output' does not exist"

This is expected - OpenAI and Gemini have different response formats. The abstraction layer handles this with type casting.

### Tool calls not working

1. Check that `GOOGLE_API_KEY` or `OPENAI_API_KEY` is set correctly
2. Verify `AI_PROVIDER` environment variable
3. Check Langfuse traces for actual API errors
4. Ensure you have sufficient credits/quota

## Future Enhancements

Potential improvements for future versions:
- [ ] Support for Anthropic Claude
- [ ] Support for Azure OpenAI
- [ ] Automatic fallback between providers
- [ ] Cost tracking per provider
- [ ] Performance comparison dashboard
- [ ] A/B testing between providers

## References

- [Google Gemini Structured Output Docs](https://ai.google.dev/gemini-api/docs/structured-output)
- [OpenAI Responses API](https://platform.openai.com/docs/api-reference/responses)
- Implementation: `lib/aiProvider.ts`, `lib/aiToolsGemini.ts`
- Intent Orchestrator: `lib/intentOrchestrator.ts`
- Analyze Endpoint: `app/api/analyze/route.ts`

